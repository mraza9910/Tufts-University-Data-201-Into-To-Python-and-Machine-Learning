{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "### Name: Maida Raza\n",
    "### Collaborator:\n",
    "\n",
    "\n",
    "DATA 201\n",
    "\n",
    "Summer 2025\n",
    "\n",
    "Tufts University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework explores KNN, Decision Trees, and Random Forests. The first question reviews training a KNN model. Subsequent questions provide an in-depth examination of Gini impurity and the mechanics of training a Decision Tree. Following this, we delve into basic implementations of both Decision Trees and Random Forests, accompanied by an introduction to tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "(a) Load the Heart Disease dataset and name it 'df'. Conduct data cleaning.\n",
    "\n",
    "- Perform any data cleaning or data transformation steps if required\n",
    "- Explain some of the data cleaning steps which you can perform on **any** data set\n",
    "\n",
    "For clarification, please find the metadata below:\n",
    "- BPMeds: whether or not the patient was on blood pressure medication\n",
    "- prevalentStroke: whether or not the patient had previously had a stroke\n",
    "- prevalentHyp: whether or not the patient was hypertensive\n",
    "- diabetes: whether or not the patient had diabetes\n",
    "- totChol: total cholesterol level\n",
    "- sysBP: systolic blood pressure\n",
    "- diaBP: diastolic blood pressure\n",
    "- BMI: Body Mass Index\n",
    "- heartRate: heart rate\n",
    "- glucose: glucose level\n",
    "- 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”) (Predictor Variable)\n",
    "\n",
    "(b) Create two dataframes for features and target variable (TenYearCHD). Conduct a full model training and testing (30%) for a KNN model with K=5. Print the accuracy, confusion matrix, and explain what the confuison matrix tells you\n",
    "\n",
    "(c) Create a range from 1 to 50 going at steps of 2 then make a list where you will store average accuracy at each k value\n",
    "use a for loop to compute the average accuracy over 10-fold cross validation for each k value. Plot the average accuracy for each k values and determine which k value you will choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>105</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currentSmoker</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cigsPerDay</th>\n",
       "      <td>29</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BPMeds</th>\n",
       "      <td>53</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevalentStroke</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevalentHyp</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totChol</th>\n",
       "      <td>50</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sysBP</th>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diaBP</th>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>19</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heartRate</th>\n",
       "      <td>1</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glucose</th>\n",
       "      <td>388</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TenYearCHD</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Missing Values Data Type  Duplicates\n",
       "male                          0     int64           0\n",
       "age                           0     int64           0\n",
       "education                   105   float64           0\n",
       "currentSmoker                 0     int64           0\n",
       "cigsPerDay                   29   float64           0\n",
       "BPMeds                       53   float64           0\n",
       "prevalentStroke               0     int64           0\n",
       "prevalentHyp                  0     int64           0\n",
       "diabetes                      0     int64           0\n",
       "totChol                      50   float64           0\n",
       "sysBP                         0   float64           0\n",
       "diaBP                         0   float64           0\n",
       "BMI                          19   float64           0\n",
       "heartRate                     1   float64           0\n",
       "glucose                     388   float64           0\n",
       "TenYearCHD                    0     int64           0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load the Heart Disease dataset and name it 'df'. Conduct data cleaning and explain \n",
    "\n",
    "df = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Heart Disease Data.csv')\n",
    "\n",
    "\n",
    "# Check for missing observations\n",
    "missing_observations = df.isnull().sum()\n",
    "\n",
    "# Check the data type for each column\n",
    "data_type = df.dtypes\n",
    "\n",
    "# Duplicated rows\n",
    "duplicates = df.duplicated().sum()\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Missing Values': missing_observations,\n",
    "    'Data Type': data_type,\n",
    "    'Duplicates': duplicates\n",
    "})\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>25.97</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>19.71</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.16</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>133.5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>21.47</td>\n",
       "      <td>80.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4238 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n",
       "0        1   39        4.0              0         0.0     0.0   \n",
       "1        0   46        2.0              0         0.0     0.0   \n",
       "2        1   48        1.0              1        20.0     0.0   \n",
       "3        0   61        3.0              1        30.0     0.0   \n",
       "4        0   46        3.0              1        23.0     0.0   \n",
       "...    ...  ...        ...            ...         ...     ...   \n",
       "4233     1   50        1.0              1         1.0     0.0   \n",
       "4234     1   51        3.0              1        43.0     0.0   \n",
       "4235     0   48        2.0              1        20.0     NaN   \n",
       "4236     0   44        1.0              1        15.0     0.0   \n",
       "4237     0   52        2.0              0         0.0     0.0   \n",
       "\n",
       "      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n",
       "0                   0             0         0    195.0  106.0   70.0  26.97   \n",
       "1                   0             0         0    250.0  121.0   81.0  28.73   \n",
       "2                   0             0         0    245.0  127.5   80.0  25.34   \n",
       "3                   0             1         0    225.0  150.0   95.0  28.58   \n",
       "4                   0             0         0    285.0  130.0   84.0  23.10   \n",
       "...               ...           ...       ...      ...    ...    ...    ...   \n",
       "4233                0             1         0    313.0  179.0   92.0  25.97   \n",
       "4234                0             0         0    207.0  126.5   80.0  19.71   \n",
       "4235                0             0         0    248.0  131.0   72.0  22.00   \n",
       "4236                0             0         0    210.0  126.5   87.0  19.16   \n",
       "4237                0             0         0    269.0  133.5   83.0  21.47   \n",
       "\n",
       "      heartRate  glucose  TenYearCHD  \n",
       "0          80.0     77.0           0  \n",
       "1          95.0     76.0           0  \n",
       "2          75.0     70.0           0  \n",
       "3          65.0    103.0           1  \n",
       "4          85.0     85.0           0  \n",
       "...         ...      ...         ...  \n",
       "4233       66.0     86.0           1  \n",
       "4234       65.0     68.0           0  \n",
       "4235       84.0     86.0           0  \n",
       "4236       86.0      NaN           0  \n",
       "4237       80.0    107.0           0  \n",
       "\n",
       "[4238 rows x 16 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Data Cleaning:\n",
    "\n",
    "1. Missing Data: Checking for missing data and deciding how to handle it (replace with mean/median). \n",
    "- Missing observations for: cigsPerDay, BPMeds, TotalCol, BMI, Heartrate, Glucose. Will replace the missing observations with the column median. I will drop the column Glucose as that has many missing observations\n",
    "\n",
    "2. Check for duplicates: Check for and remove any duplicates\n",
    "- No duplicates were found in any features\n",
    "\n",
    "3. Data Type: Ensure that columns with string/integer variables have consistent data type\n",
    "- Data type seems consistent for every feature\n",
    "\n",
    "4. Outlier Detection: Identify and address outliers\n",
    "\n",
    "5. Dropping Irrelevant Features: Remove the columns not useful for our analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male               0\n",
       "age                0\n",
       "education          0\n",
       "currentSmoker      0\n",
       "cigsPerDay         0\n",
       "BPMeds             0\n",
       "prevalentStroke    0\n",
       "prevalentHyp       0\n",
       "diabetes           0\n",
       "totChol            0\n",
       "sysBP              0\n",
       "diaBP              0\n",
       "BMI                0\n",
       "heartRate          0\n",
       "glucose            0\n",
       "TenYearCHD         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filling in missing observations with their column median\n",
    "df.fillna(df.median(numeric_only=True), inplace=True) # putting inplace because we want to replace values without creating new copies\n",
    "df.drop('glucose', axis='columns') # need to specify axis = 'rows/column\n",
    "\n",
    "# Verify that all columns have been cleaned\n",
    "cleaned_df = df.isnull().sum()\n",
    "cleaned_df \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The data cleaning process is complete. All missing values have been filled using the median, and there are no remaining missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix is [[1045   39]\n",
      " [ 168   20]]\n",
      "Accuracy Score is 0.8372641509433962\n"
     ]
    }
   ],
   "source": [
    "# (b) Create two dataframes for features and target variable (TenYearCHD). In here, I will assume that all other columns are the predictor variables\n",
    "# Conduct a full model training and testing (30%) for a KNN model with K=5. Print the accuracy, confusion matrix, and explain what the confusion matrix tells you\n",
    "\n",
    "y = df['TenYearCHD']\n",
    "x = df.drop('TenYearCHD', axis= 'columns')\n",
    "\n",
    "# Standardize the feature data - transforming the data to have zero mean and unit variance. This helps improve performance for algorithms like KNN, Vector Machines, and Gradient Boost.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(x)\n",
    "\n",
    "# Dividing into test and train data\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Now running the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(x_train, y_train)\n",
    "y_predict = knn_model.predict(x_test)\n",
    "\n",
    "# Calculating confusion matrix and the accuracy score\n",
    "con_mat = confusion_matrix(y_test, y_predict)\n",
    "acc_score = accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(f'Confusion Matrix is {con_mat}')\n",
    "print(f'Accuracy Score is {acc_score}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1(b) Result Analysis:\n",
    "\n",
    "1. Accuracy Score:\n",
    "- The model has an accuracy score of 0.8372641509433962, meaning that the KNN model achieved an accuracy of 83.7% on the test dataset.\n",
    "\n",
    "2. Confusion Matrix: I will assume that negative means the patient not getting CHD (0) and positive means the patient getting CHD (1)\n",
    "\n",
    "- Here we were using the KNN model to predict the risk of coronary heart disease, given a sample of data. Our model learned from the x_train dataset to create predictions shown in the Confusion Matrix above.\n",
    "- True Negative: KNN model accurately predicts 1045 instances of patient not getting CHD (0). \n",
    "- False Positive: KNN model inaccurately predicted 39 instances of patient getting CHD (1) when the actual outcome is 0.\n",
    "- False Negative: KNN model inaccurately predicted that 168 patients wont get CHD when the actual outcome is 1, meaning that they will get CHD\n",
    "- True Positive: KNN model accurately predicted 20 patient getting CHD, who will actually get it.\n",
    "\n",
    "The model performs well in determining individuals without risk, but it struggles with identifying individuals at risk of getting CHD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q1 (b)\n",
    "\n",
    "Model Results:\n",
    "* Accuracy: The KNN model achieved an accuracy of 83.49% on the test dataset.\n",
    "\n",
    "Confusion Matrix Explanation:\n",
    "* True Negatives (1042): The model correctly predicted 1042 instances as \"No risk\" (0) for TenYearCHD.\n",
    "* False Positives (35): The model incorrectly predicted \"Yes risk\" (1) for 35 instances that were actually \"No risk\" (0).\n",
    "* False Negatives (175): The model incorrectly predicted \"No risk\" (0) for 175 instances that were actually \"Yes risk\" (1).\n",
    "* True Positives (20): The model correctly predicted 20 instances as \"Yes risk\" (1) for TenYearCHD.\n",
    "\n",
    "The model has good performance in identifying individuals without the risk, but it struggles more with correctly identifying those who are at risk (low number of true positives). This suggests potential issues with imbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a range from 1 to 50 going at steps of 2 then make a list where you will store average accuracy at each k value\n",
    "# use a for loop to compute the average accuracy over 10-fold cross validation for each k value. Plot the average accuracy for each k values and determine which k value you will choose.\n",
    "# I am also curious to know if 10 cross validation is better than 5 fold cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score # evaluates model performance through cross validation, divides the data into test and split and calculates the score. \n",
    "k_values = range(1,51,2)\n",
    "average_accuracy_10 = []\n",
    "average_accuracy_5 = []\n",
    "k_vals = []\n",
    "\n",
    "for k in k_values:\n",
    "    k_vals.append(k)\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    score_10 = cross_val_score(model, scaled_data, y, cv=10)\n",
    "    score_5 = cross_val_score(model, scaled_data, y, cv=5)\n",
    "    average_accuracy_10.append(score_10.mean())\n",
    "    average_accuracy_5.append(score_5.mean())\n",
    "    \n",
    "# Creating the plot now - essentially we are trying to see which k-values give us the best score\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(k_vals, average_accuracy_10, marker = 'o', color = 'orange', label = '10 Cross Fold Validation')\n",
    "plt.plot(k_vals, average_accuracy_5, marker = 'x', color = 'black', label = '5 Cross Fold Validation')\n",
    "plt.title('Average Model Accuracy for Different K Values (KNN)')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Average Model Accuracy')\n",
    "plt.xticks(k_vals)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q1 (c)\n",
    "\n",
    "The plot shows the average accuracy for different K values in the KNN model. Based on the results, the best K value is the one that maximizes the average accuracy. In this case, the optimal K value is:\n",
    "\n",
    "K = 13\n",
    "\n",
    "This K value provides the highest average accuracy over 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "The goal of Q2 is to make a function that can calculate the weighted gini impurity over any grouping and any size of classes.\n",
    "\n",
    "(a) Calculate the gini impurity of the example by hand. Then write some code to do it for you. You will have to find \n",
    " and \n",
    " which are the probabilities of selecting a 0 and a 1 (respectively) from the group.\n",
    "\n",
    "(b) Now say we have two groups? Calculate the gini index of each group. Now, make a weighted sum of the gini numbers, each weighted by the proportion of the group size to the total number of entries (i.e. if group 1 is of length 10 and group 2 is of length 15, then group 1 would have a weight of 2/5 and group 2 a weight of 3/5).\n",
    "\n",
    "(c) Generalize that bit of code you wrote to now deal with any number of groups with any number of classes. Assume the groups will be given as a list of lists, and if there is an empty group the gini number will be 0 (there needs to be an if statement to make sure we do not divide by 0).\n",
    "\n",
    "(d) Make that bit of code into a function called gini_imp which takes a 'groups' variable and a 'classes' variable. It should return the gini index. Note, you need to use the classes variable because that is how you know what classes to check for in each group. Test it on the same example from part (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2 (a)\n",
    "\n",
    "###This is the example\n",
    "classes = [0,1]\n",
    "group = [0,0,0,1,1,0,1,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555556"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2(a) Gini Impurity tells us how often a randomly chosen data point would be misclassified\n",
    "# Code for calculating Gini Impurity for the data above:\n",
    "# This function takes the total length of a group, accesses the data observations inside it and then assign probabilities\n",
    "\n",
    "def gini_impurity(group):\n",
    "\n",
    "    sample_size = len(group)\n",
    "    zeros = ([g for g in group if g == 0])\n",
    "    ones = ([m for m in group if m == 1])\n",
    "\n",
    "    p_0 = (len(zeros) / sample_size) ** 2\n",
    "    p_1 = (len(ones) / sample_size) ** 2\n",
    "    \n",
    "    Gini_Index = p_0 + p_1\n",
    "    Gini_Impurity = 1 - (p_0 + p_1)\n",
    "    return Gini_Index\n",
    "\n",
    "gini_impurity([0,0,0,1,1,0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q2 (a)\n",
    "\n",
    "The Gini Impurity measures the impurity or heterogeneity of a group. It is calculated using the formula:\n",
    "\n",
    "Gini Impurity = 1 - (p0^2 + p1^2)\n",
    "\n",
    "Where:\n",
    "* p0 is the probability of selecting a 0 from the group.\n",
    "* p1 is the probability of selecting a 1 from the group.\n",
    "\n",
    "Given the group: [0, 0, 0, 1, 1, 0, 1, 0, 0]\n",
    "* Total number of elements: n = 9\n",
    "* Number of 0's: n0 = 6\n",
    "* Number of 1's: n1 = 3\n",
    "\n",
    "Now we can find the probabilities:\n",
    "* p0 = n0/n = 6/9 = 0.6667\n",
    "* p1 = n1/n = 3/9 = 0.3333\n",
    "\n",
    "Using the formula Gini Impurity = 1 - (p0^2 + p1^2):\n",
    "* Gini Impurity = 1 - (0.6667^2 + 0.3333^2)\n",
    "* Gini Impurity = 1 - (0.4444 + 0.1111) = 1 - 0.5555 = 0.4444\n",
    "\n",
    "Thus, the Gini Impurity of the example is 0.4444.\n",
    "\n",
    "Gini impurity = 0 (perfectly pure)\n",
    "Gini Impurity = 0.5 (Maximum Impurity)\n",
    "Gini Impurity > 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4375"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2(b) Now say we have two groups. Calculate the gini index of each group. Now, make a weighted sum of the gini numbers, each weighted by the proportion of the group size to the total number of entries \n",
    "# # (i.e. if group 1 is of length 10 and group 2 is of length 15, then group 1 would have a weight of 2/5 and group 2 a weight of 3/5).\n",
    "\n",
    "classes = [0,1]\n",
    "groups =([0, 0, 0, 1], [0, 0, 1, 1])\n",
    "\n",
    "def gini_impurity(groups):\n",
    "    total_length = 0\n",
    "    total_weighted_gini = []\n",
    "    gini_index = 0\n",
    "    gini_sum = 0\n",
    "    \n",
    "    for m in groups:\n",
    "        total_length = len(m) + total_length\n",
    "    \n",
    "    for i in groups:\n",
    "        group_len = len(i)\n",
    "        zeros = i.count(0)\n",
    "        ones = i.count(1)\n",
    "        p_0 = (zeros / group_len ) **2\n",
    "        p_1 = (ones/group_len) ** 2\n",
    "        gini_index = (1 - (p_0 + p_1))\n",
    "        weight = group_len / total_length\n",
    "        weighted_gini_index = (weight) * gini_index\n",
    "        total_weighted_gini.append(weighted_gini_index)\n",
    "        \n",
    "    for p in total_weighted_gini:\n",
    "        gini_sum = p + gini_sum\n",
    "    return gini_sum\n",
    "\n",
    "gini_impurity(groups)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5684210526315789"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2(c) Generalize that bit of code you wrote to now deal with any number of groups with any number of classes. \n",
    "# Assume the groups will be given as a list of lists, and if there is an empty group the gini number will be 0 (there needs to be an if statement to make sure we do not divide by 0).\n",
    "\n",
    "classes = [0,1,2,3]\n",
    "groups =[[0,3,1,1,1],[0,0,0,1],[2,3,1,1],[],[0,0,1,1,2,2]]\n",
    "\n",
    "def gini_impurity(groups, classes):\n",
    "    \n",
    "    total_length = 0\n",
    "    total_weighted_gini = []\n",
    "    gini_index = 0\n",
    "    gini_sum = 0\n",
    "    \n",
    "    for m in groups:\n",
    "        total_length = len(m) + total_length # you are getting the total length of the set\n",
    "    print(total_length) # total length, i.e., data observations in the set. For coder checking purposes only\n",
    "    \n",
    "    for i in groups:\n",
    "        if len(i) != 0:\n",
    "            group_length = len(i)\n",
    "            probabilities = [i.count(c) for c in classes]\n",
    "            group_weight = group_length / total_length\n",
    "        \n",
    "            for p in probabilities:\n",
    "                calculation = (p/group_length) ** 2\n",
    "                gini = calculation * group_weight\n",
    "                gini_index = gini_index + gini\n",
    "        else:\n",
    "            gini_index = 0 + gini_index  \n",
    "    return 1-gini_index\n",
    "\n",
    "gini_impurity(groups, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Gini Index: 0.5684210526315789\n"
     ]
    }
   ],
   "source": [
    "# 2(d) Make that bit of code into a function called gini_imp which takes a 'groups' variable and a 'classes' variable. \n",
    "# It should return the gini index. Note, you need to use the classes variable because that is how you know what classes to check for in each group. Test it on the same example from part (c)\n",
    "\n",
    "def gini_imp(groups, classes):\n",
    "    def gini_impurity(groups, classes):\n",
    "    \n",
    "        total_length = 0\n",
    "        total_weighted_gini = []\n",
    "        gini_sum = 0\n",
    "        \n",
    "        for m in groups:\n",
    "            total_length = len(m) + total_length # you are getting the total length of the set\n",
    "        print(total_length) # total length, i.e., data observations in the set. For coder checking purposes\n",
    "        \n",
    "        for i in groups:\n",
    "            if len(i) != 0:\n",
    "                group_length = len(i)\n",
    "                probabilities = [i.count(c) for c in classes]\n",
    "                group_weight = group_length / total_length\n",
    "            \n",
    "                for p in probabilities:\n",
    "                    calculation = (p/group_length) ** 2\n",
    "                    gini = calculation * group_weight\n",
    "                    gini_sum = gini_sum + gini\n",
    "            else:\n",
    "                gini_sum = 0 + gini_sum \n",
    "        return(1-gini_sum)\n",
    "    return gini_impurity(groups, classes)\n",
    "\n",
    "gini_index = gini_imp(groups, classes)\n",
    "print(f'Gini Index:', gini_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "To make a full decision trees\n",
    "\n",
    "(a) Load the datasets titanic_train_data and titanic_test_data into dataframes and split them into X_train, X_test, y_train and y_test (the training sets are coming from the titanic train data and visa versa). Note, the first two columns should be deleted because they are just passenger Id's, also documentation about the data can be found here:https://www.kaggle.com/azeembootwala/titanic. Create a Decision Tree instance, fit the data on the training sets and get the accuracy score on the test set. Report the confusion matrix.\n",
    "\n",
    "(b) Split the X_train and y_train data into sets called X_train,X_val, y_train, y_val using train_test_split. We are going to try and tune some parameters in the Decision tree. Called a new Decision Tree instance with the following parameters: **max_features, max_depth, and min_samples_leaf**. Choose some value to test them with, and run the model a few times to see if you can get different accuracy scores (use the X_val and y_val to get the accuracy scores). What do these parameters do? (Look them up in the documentation)\n",
    "\n",
    "(c) We are going to use the validation sets to try and find the best parameter combinations. So, use a triple for loop to iterate over different ranges for each of the three parameters, find what combination gives the best accuracy on the validation set. Then, use that combination on a decision tree to classify the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score for Decision Tree Model is 0.8\n",
      "The Confusion Matrix for Decision Tree Model is [[52 12]\n",
      " [ 8 28]]\n"
     ]
    }
   ],
   "source": [
    "# 3(a) (a) Load the datasets titanic_train_data and titanic_test_data into dataframes and split them into X_train, X_test, y_train and y_test (the training sets are coming from the titanic train data and visa versa).\n",
    "# Note, the first two columns should be deleted because they are just passenger Id's, also documentation about the data can be found here:https://www.kaggle.com/azeembootwala/titanic. \n",
    "# Create a Decision Tree instance, fit the data on the training sets and get the accuracy score on the test set. Report the confusion matrix.\n",
    "'Decision Tree models can be used to analyze whether a customer will purchase a product based on their age, income, and purchase history.'\n",
    "'In this one, you may start with income > 100k if advertising an expensive product. Decision tree can also be used for credit scoring and fraud detection'\n",
    "'Tree Depth is the number of steps the algorithm takes to make a decision'\n",
    "\n",
    "#Load the necessary libraries:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the datasets\n",
    "train = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/train_data.csv')\n",
    "test = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/test_data.csv')\n",
    "\n",
    "#Checking if observations are missing\n",
    "\n",
    "train_missing_observations = train.isnull().sum()\n",
    "test_missing_observations = train.isnull().sum()\n",
    "\n",
    "train_duplicates = train.duplicated()\n",
    "test_duplicates = test.duplicated()\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'missing_observations': train_missing_observations,\n",
    "    'test missing obs': test_missing_observations,\n",
    "    'train duplicates': train_duplicates,\n",
    "    'test duplicates': test_duplicates\n",
    "})\n",
    "\n",
    "\n",
    "# Dropping the first two columns\n",
    "train = train.iloc[:, 2:]\n",
    "test = test.iloc[:, 2:]\n",
    "\n",
    "# Dividing the features\n",
    "x_train = train.drop('Survived', axis=1)\n",
    "y_train = train['Survived']\n",
    "\n",
    "x_test = test.drop('Survived', axis =1)\n",
    "y_test = test['Survived']\n",
    "\n",
    "# Building the Decision Tree Model\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(x_train, y_train)\n",
    "y_test_predict = tree.predict(x_test)\n",
    "\n",
    "# Getting Accuracy Score and Confusion Matrix on the Test Dataset\n",
    "accuracy_score = accuracy_score(y_test, y_test_predict)\n",
    "confusion_matrix = confusion_matrix(y_test, y_test_predict)\n",
    "\n",
    "# Printing the output\n",
    "print(f'The Accuracy Score for Decision Tree Model is {accuracy_score}')\n",
    "print(f'The Confusion Matrix for Decision Tree Model is {confusion_matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of output - 3(a)\n",
    "\n",
    "1. Accuracy Score: \n",
    "\n",
    "- The accuracy of the decision tree model on the titanic dataset is 0.8, meaning that the model is making correct predictions about survival 80% of the time\n",
    "\n",
    "2. Confusion Matrix\n",
    "\n",
    "- True Negative: 52 - for 52 observations, our model correctly identified people as not surviving\n",
    "- False Positive: 12 - for 12 observations, our model predicted people to survive, but they did not\n",
    "- False Negative: 8 - predicted 8 passengers to not survive when they actually survived\n",
    "- True Positive: 28 - predicted 28 passenger to survive when they actually survive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792 792\n",
      "[[229  16]\n",
      " [ 41 110]] 0.8560606060606061\n"
     ]
    }
   ],
   "source": [
    "# 3(b) (b) Split the X_train and y_train data into sets called X_train,X_val, y_train, y_val using train_test_split. \n",
    "# We are going to try and tune some parameters in the Decision tree. Called a new Decision Tree instance with the following parameters: \n",
    "# **max_features, max_depth, and min_samples_leaf**. Choose some value to test them with, and run the model a few times to see if you can get different accuracy scores (use the X_val and y_val to get the accuracy scores). \n",
    "# What do these parameters do? (Look them up in the documentation)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x_train = train.drop('Survived', axis =1)\n",
    "y_train = train['Survived']\n",
    "print(len(x_train), len(y_train))\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "# Building New Decision Tree instance\n",
    "# Max Depth: determines how many steps a decision tree takes. Note that the higher the depth, there is a chance of overfitting because the model trained too well\n",
    "# Max Features: you set a number of features that the model should consider. The model, now decides which features are important. \n",
    "# Decision trees can overemphasize high-cardinality categorical features or numerical features with many possible splits\n",
    "# Min Sample Leaf: A leaf note is the one where the tree stops and makes a prediction. Essentially, specifies the minimum number of observations that should be there before prediction\n",
    "\n",
    "New_Tree = DecisionTreeClassifier(max_features='log2', max_depth=5, min_samples_leaf=8)\n",
    "New_Tree.fit(x_train, y_train)\n",
    "y_predict = New_Tree.predict(X_val)\n",
    "\n",
    "acc_score = accuracy_score(Y_val, y_predict)\n",
    "conf_matrix = confusion_matrix(Y_val, y_predict)\n",
    "print(conf_matrix, acc_score)\n",
    "\n",
    "\n",
    "# Printing the accuracy score\n",
    "#print(f'Accuracy Score under Decision Tree Model (max_features=None, max_depth=5, min_samples_leaf=1) is {acc_score}') 0.87\n",
    "#print(f'Accuracy Score under Decision Tree Model (max_features=None, max_depth=5, min_samples_leaf=8) is {acc_score}') 0.85 -> Score went down because I kept everything same, but required the model to have more instances before making a prediction\n",
    "#print(f'Accuracy Score under Decision Tree Model (max_features=8, max_depth=8, min_samples_leaf=1) is {acc_score}') 0.89\n",
    "#print(f'Accuracy Score under Decision Tree Model (max_features=None, max_depth=5, min_samples_leaf=8) is {acc_score}') 0.85\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q3 (b)\n",
    "\n",
    "With the chosen parameters (max_features='log2', max_depth=5, and min_samples_leaf=8), the accuracy score on the validation set is 85.6%.\n",
    "\n",
    "Explanation of the Parameters:\n",
    "* max_features: This parameter controls the number of features considered for splitting at each node. Using SQRT means that we are limiting the model to only consider a few features. This has the risk of both over and under fitting because the model may drop/consider features important/not important\n",
    "* max_depth: This parameter sets the maximum depth of the tree. Limiting the depth can prevent the model from becoming overly complex, reducing the risk of overfitting.\n",
    "* min_samples_leaf: This defines the minimum number of samples required to be at a leaf node. By setting a higher value, we ensure that leaf nodes have more samples, which helps in creating a more generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792 792\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 3, min_samples_leaf= 6 is 0.7853535353535354\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 3, min_samples_leaf= 7 is 0.7752525252525253\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 3, min_samples_leaf= 8 is 0.8358585858585859\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 3, min_samples_leaf= 9 is 0.8308080808080808\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 3, min_samples_leaf= 10 is 0.7929292929292929\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 4, min_samples_leaf= 6 is 0.8409090909090909\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 4, min_samples_leaf= 7 is 0.8156565656565656\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 4, min_samples_leaf= 8 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 4, min_samples_leaf= 9 is 0.7626262626262627\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 4, min_samples_leaf= 10 is 0.8333333333333334\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 5, min_samples_leaf= 6 is 0.8156565656565656\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 5, min_samples_leaf= 7 is 0.8080808080808081\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 5, min_samples_leaf= 8 is 0.7929292929292929\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 5, min_samples_leaf= 9 is 0.8333333333333334\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 5, min_samples_leaf= 10 is 0.8232323232323232\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 6, min_samples_leaf= 6 is 0.8282828282828283\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 6, min_samples_leaf= 7 is 0.8358585858585859\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 6, min_samples_leaf= 8 is 0.8585858585858586\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 6, min_samples_leaf= 9 is 0.8333333333333334\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 6, min_samples_leaf= 10 is 0.8106060606060606\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 7, min_samples_leaf= 6 is 0.8409090909090909\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 7, min_samples_leaf= 7 is 0.8585858585858586\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 7, min_samples_leaf= 8 is 0.8358585858585859\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 7, min_samples_leaf= 9 is 0.8409090909090909\n",
      "The Accuracy Score for Decision Tree at max_features= 3, max_depth= 7, min_samples_leaf= 10 is 0.8535353535353535\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 3, min_samples_leaf= 6 is 0.8282828282828283\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 3, min_samples_leaf= 7 is 0.7878787878787878\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 3, min_samples_leaf= 8 is 0.8156565656565656\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 3, min_samples_leaf= 9 is 0.8459595959595959\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 3, min_samples_leaf= 10 is 0.7373737373737373\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 4, min_samples_leaf= 6 is 0.7853535353535354\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 4, min_samples_leaf= 7 is 0.8156565656565656\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 4, min_samples_leaf= 8 is 0.797979797979798\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 4, min_samples_leaf= 9 is 0.7146464646464646\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 4, min_samples_leaf= 10 is 0.7828282828282829\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 5, min_samples_leaf= 6 is 0.8055555555555556\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 5, min_samples_leaf= 7 is 0.8434343434343434\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 5, min_samples_leaf= 8 is 0.8156565656565656\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 5, min_samples_leaf= 9 is 0.8257575757575758\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 5, min_samples_leaf= 10 is 0.7954545454545454\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 6, min_samples_leaf= 6 is 0.8181818181818182\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 6, min_samples_leaf= 7 is 0.8333333333333334\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 6, min_samples_leaf= 8 is 0.8207070707070707\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 6, min_samples_leaf= 9 is 0.8636363636363636\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 6, min_samples_leaf= 10 is 0.7878787878787878\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 7, min_samples_leaf= 6 is 0.8585858585858586\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 7, min_samples_leaf= 7 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 7, min_samples_leaf= 8 is 0.8434343434343434\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 7, min_samples_leaf= 9 is 0.8333333333333334\n",
      "The Accuracy Score for Decision Tree at max_features= 4, max_depth= 7, min_samples_leaf= 10 is 0.8131313131313131\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 3, min_samples_leaf= 6 is 0.7272727272727273\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 3, min_samples_leaf= 7 is 0.7878787878787878\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 3, min_samples_leaf= 8 is 0.8358585858585859\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 3, min_samples_leaf= 9 is 0.8181818181818182\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 3, min_samples_leaf= 10 is 0.8358585858585859\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 4, min_samples_leaf= 6 is 0.7954545454545454\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 4, min_samples_leaf= 7 is 0.7929292929292929\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 4, min_samples_leaf= 8 is 0.7752525252525253\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 4, min_samples_leaf= 9 is 0.8484848484848485\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 4, min_samples_leaf= 10 is 0.8106060606060606\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 5, min_samples_leaf= 6 is 0.8459595959595959\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 5, min_samples_leaf= 7 is 0.8383838383838383\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 5, min_samples_leaf= 8 is 0.8611111111111112\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 5, min_samples_leaf= 9 is 0.8207070707070707\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 5, min_samples_leaf= 10 is 0.8106060606060606\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 6, min_samples_leaf= 6 is 0.8636363636363636\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 6, min_samples_leaf= 7 is 0.8282828282828283\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 6, min_samples_leaf= 8 is 0.8611111111111112\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 6, min_samples_leaf= 9 is 0.8535353535353535\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 6, min_samples_leaf= 10 is 0.8207070707070707\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 7, min_samples_leaf= 6 is 0.8762626262626263\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 7, min_samples_leaf= 7 is 0.8383838383838383\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 7, min_samples_leaf= 8 is 0.8282828282828283\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 7, min_samples_leaf= 9 is 0.8207070707070707\n",
      "The Accuracy Score for Decision Tree at max_features= 5, max_depth= 7, min_samples_leaf= 10 is 0.8207070707070707\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 3, min_samples_leaf= 6 is 0.7727272727272727\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 3, min_samples_leaf= 7 is 0.8106060606060606\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 3, min_samples_leaf= 8 is 0.8106060606060606\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 3, min_samples_leaf= 9 is 0.7929292929292929\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 3, min_samples_leaf= 10 is 0.8282828282828283\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 4, min_samples_leaf= 6 is 0.8156565656565656\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 4, min_samples_leaf= 7 is 0.8409090909090909\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 4, min_samples_leaf= 8 is 0.8611111111111112\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 4, min_samples_leaf= 9 is 0.8055555555555556\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 4, min_samples_leaf= 10 is 0.851010101010101\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 5, min_samples_leaf= 6 is 0.8383838383838383\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 5, min_samples_leaf= 7 is 0.8358585858585859\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 5, min_samples_leaf= 8 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 5, min_samples_leaf= 9 is 0.8005050505050505\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 5, min_samples_leaf= 10 is 0.8585858585858586\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 6, min_samples_leaf= 6 is 0.8308080808080808\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 6, min_samples_leaf= 7 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 6, min_samples_leaf= 8 is 0.8434343434343434\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 6, min_samples_leaf= 9 is 0.8661616161616161\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 6, min_samples_leaf= 10 is 0.8333333333333334\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 7, min_samples_leaf= 6 is 0.8661616161616161\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 7, min_samples_leaf= 7 is 0.8762626262626263\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 7, min_samples_leaf= 8 is 0.8686868686868687\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 7, min_samples_leaf= 9 is 0.8333333333333334\n",
      "The Accuracy Score for Decision Tree at max_features= 6, max_depth= 7, min_samples_leaf= 10 is 0.8358585858585859\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 3, min_samples_leaf= 6 is 0.8106060606060606\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 3, min_samples_leaf= 7 is 0.7121212121212122\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 3, min_samples_leaf= 8 is 0.8055555555555556\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 3, min_samples_leaf= 9 is 0.7575757575757576\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 3, min_samples_leaf= 10 is 0.8358585858585859\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 4, min_samples_leaf= 6 is 0.803030303030303\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 4, min_samples_leaf= 7 is 0.8383838383838383\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 4, min_samples_leaf= 8 is 0.8131313131313131\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 4, min_samples_leaf= 9 is 0.8131313131313131\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 4, min_samples_leaf= 10 is 0.7752525252525253\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 5, min_samples_leaf= 6 is 0.8434343434343434\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 5, min_samples_leaf= 7 is 0.8409090909090909\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 5, min_samples_leaf= 8 is 0.8282828282828283\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 5, min_samples_leaf= 9 is 0.8409090909090909\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 5, min_samples_leaf= 10 is 0.7929292929292929\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 6, min_samples_leaf= 6 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 6, min_samples_leaf= 7 is 0.803030303030303\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 6, min_samples_leaf= 8 is 0.851010101010101\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 6, min_samples_leaf= 9 is 0.8282828282828283\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 6, min_samples_leaf= 10 is 0.8358585858585859\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 7, min_samples_leaf= 6 is 0.8661616161616161\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 7, min_samples_leaf= 7 is 0.8358585858585859\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 7, min_samples_leaf= 8 is 0.8409090909090909\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 7, min_samples_leaf= 9 is 0.8131313131313131\n",
      "The Accuracy Score for Decision Tree at max_features= 7, max_depth= 7, min_samples_leaf= 10 is 0.8409090909090909\n"
     ]
    }
   ],
   "source": [
    "# 3(c) c) We are going to use the validation sets to try and find the best parameter combinations. \n",
    "# So, use a triple for loop to iterate over different ranges for each of the three parameters, find what combination gives the best accuracy on the validation set. \n",
    "# Then, use that combination on a decision tree to classify the test set.\n",
    "\n",
    "x_train = train.drop('Survived', axis =1)\n",
    "y_train = train['Survived']\n",
    "print(len(x_train), len(y_train))\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "# Building New Decision Tree instance\n",
    "# Max Depth: determines how many steps a decision tree takes. Note that the higher the depth, there is a chance of overfitting because the model trained too well\n",
    "# Max Features: you set a number of features that the model should consider. The model, now decides which features are important. \n",
    "# Decision trees can overemphasize high-cardinality categorical features or numerical features with many possible splits\n",
    "# Min Sample Leaf: A leaf note is the one where the tree stops and makes a prediction. Essentially, specifies the minimum number of observations that should be there before prediction\n",
    "\n",
    "for i in range(3,8):\n",
    "    for m in range(3,8):\n",
    "        for j in range(6,11):\n",
    "            New_Tree = DecisionTreeClassifier(max_features='sqrt', max_depth=m, min_samples_leaf=j)\n",
    "            New_Tree.fit(x_train, y_train)\n",
    "            y_predict = New_Tree.predict(X_val)\n",
    "            acc_score = accuracy_score(Y_val, y_predict)\n",
    "            print(f'The Accuracy Score for Decision Tree at max_features= {i}, max_depth= {m}, min_samples_leaf= {j} is {acc_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792 792\n",
      "Best Parameters Found: {'min_samples_leaf': 8, 'max_features': 'sqrt', 'max_depth': 7}\n"
     ]
    }
   ],
   "source": [
    "# Another way of determining the best parameters\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "x_train = train.drop('Survived', axis =1)\n",
    "y_train = train['Survived']\n",
    "print(len(x_train), len(y_train))\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "# Defining the dataframe\n",
    "param_dist = {\n",
    "    'max_depth': list(range(3,8)),\n",
    "    'max_features': ['sqrt', None, 'log2'],\n",
    "    'min_samples_leaf': list(range(6,11)) \n",
    "}\n",
    "\n",
    "# Defining the model\n",
    "model = DecisionTreeClassifier()\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator = model,\n",
    "    param_distributions = param_dist,\n",
    "    cv =3,\n",
    "    n_iter = 20,\n",
    "    n_jobs = 1,\n",
    "    scoring = 'accuracy'\n",
    ")\n",
    "\n",
    "# Fitting the model\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameter\n",
    "print('Best Parameters Found:', random_search.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q3 (c)\n",
    "\n",
    "The best parameter combination found on the validation set is:\n",
    "* max_features: sqrt\n",
    "* max_depth: 7\n",
    "* min_samples_leaf: 8\n",
    "\n",
    "This achieved a validation accuracy of 86.3%.\n",
    "\n",
    "Now, I will use this combination to fit a Decision Tree on the training data and evaluate its performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792 792\n",
      "Confusion Matrix [[230  15]\n",
      " [ 39 112]]\n",
      "Accuracy Score = 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "# **max_features, max_depth, and min_samples_leaf**. Choose some value to test them with, and run the model a few times to see if you can get different accuracy scores (use the X_val and y_val to get the accuracy scores). \n",
    "# What do these parameters do? (Look them up in the documentation)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_train = train.drop('Survived', axis =1)\n",
    "y_train = train['Survived']\n",
    "print(len(x_train), len(y_train))\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "# Building New Decision Tree instance\n",
    "# Max Depth: determines how many steps a decision tree takes. Note that the higher the depth, there is a chance of overfitting because the model trained too well\n",
    "# Max Features: you set a number of features that the model should consider. The model, now decides which features are important. \n",
    "# Decision trees can overemphasize high-cardinality categorical features or numerical features with many possible splits\n",
    "# Min Sample Leaf: A leaf note is the one where the tree stops and makes a prediction. Essentially, specifies the minimum number of observations that should be there before prediction\n",
    "\n",
    "New_Tree = DecisionTreeClassifier(max_features='sqrt', max_depth=7, min_samples_leaf=8)\n",
    "New_Tree.fit(x_train, y_train)\n",
    "y_predict = New_Tree.predict(X_val)\n",
    "\n",
    "acc_score = accuracy_score(Y_val, y_predict)\n",
    "conf_matrix = confusion_matrix(Y_val, y_predict)\n",
    "\n",
    "print(f'Confusion Matrix {conf_matrix}')\n",
    "print(f'Accuracy Score = {acc_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q3 (c)\n",
    "\n",
    "Using the best parameter combination, the accuracy on the test set is 86.3%\n",
    "\n",
    "The confusion matrix indicates:\n",
    "* 230 true negatives compared with 229 for the prior model (correctly predicted as not survived)\n",
    "* 15 false positives compared with 16 for the prior model (incorrectly predicted as survived)\n",
    "* 39 false negatives compared with 41 in the prior model (incorrectly identified as not surviving)\n",
    "* 112 true positive compared with 110 for the prior model/ \n",
    "\n",
    "This model performs well in both Confusion Matrix and Accuracy score when compared with the tuned Decision Tree in 3b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "Let's move onto random forests, we'll be doing more parameter tuning here.\n",
    "\n",
    "(a) With the original train and test sets, run a Random Forest model on the data and report the accuracy score. How does it compare to the scores in Question 3?\n",
    "\n",
    "(b) Create the train and validation sets again, and create a Random Forest Classifier with the following parameters: **n_estimators, max_leaf_nodes, max_depth**, with some values. What do this parameters do? (Again look up the documentation) Run the model and see how the accuracy changes. Change the values and try to get a higher accuracy.\n",
    "\n",
    "(c) Similar to Question 3 part (c), use a triple for loop to iterate over combinations of parameter values for the random forest and find one that is optimal in accuracy. How does this accuracy compare to the others we have seen?\n",
    "\n",
    "Note: This kind of parameter opimization can be done using built in python functions, GridSearchCV and RandomSearchCV both are methods that take in some kind of range / distribution for the parameters and finds the best one (and uses cross validation which is a bonus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Notes:\n",
    "\n",
    "- Built from Decision Trees\n",
    "- Trees are not flexible when it comes to classifying random samples\n",
    "- You are selecting features and other things to build the boot-strapped dataset randomly\n",
    "- Random Forests create multiple trees wit varying variables, which helps with its accuracy\n",
    "- Then you run the data on every model, and determine yes and no outcomes for your hypothesis question\n",
    "- Bagging: bootstrapping the data + using the data to make a decision. Typically, ~ 1/3 of the data does not end up in the test data. Out of bag dataset is what its called\n",
    "- We can measure the accuracy of our Random Forest by the proportion of Out-of-Bag samples that were correctly classified by the RF. Incorrectly identified -> Out of Bag Error\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score for Random Forest Model is 0.86\n",
      "The Confusion Matrix for Random Forest Model is [[58  6]\n",
      " [ 8 28]]\n"
     ]
    }
   ],
   "source": [
    "# 4(a) With the original train and test sets, run a Random Forest model on the data and report the accuracy score. How does it compare to the scores in Question 3?\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the datasets\n",
    "titanic_train = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/train_data.csv')\n",
    "titanic_test = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/test_data.csv')\n",
    "\n",
    "# Dropping the first two columns from the dataset\n",
    "titanic_train = titanic_train.iloc[:, 2:]\n",
    "titanic_test = titanic_test.iloc[:, 2:]\n",
    "\n",
    "# Dividing the dataset into features and target variable\n",
    "x_train = titanic_train.drop('Survived', axis=1)\n",
    "y_train = titanic_train['Survived']\n",
    "\n",
    "x_test = titanic_test.drop('Survived', axis=1)\n",
    "y_test = titanic_test['Survived']\n",
    "\n",
    "# Building the Random Forest Model\n",
    "model = RandomForestClassifier(n_estimators=100) \n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "# Calculating Accuracy Score and Confusion Matrix\n",
    "acc_score = accuracy_score(y_test, y_predict)\n",
    "conf_matrix = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "print(f'The Accuracy Score for Random Forest Model is {acc_score}')\n",
    "print(f'The Confusion Matrix for Random Forest Model is {conf_matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q4 (a)\n",
    "\n",
    "Using the original train and test sets, the Random Forest model achieved an accuracy of 86% on the test set.\n",
    "\n",
    "Comparison:\n",
    "* Random Forest: 86%% accuracy\n",
    "* Tuned Decision Tree: 86.3% accuracy\n",
    "* Untuned Decision Tree: 80% accuracy\n",
    "\n",
    "\n",
    "The Random Forest slightly outperformed the Decision Tree models, highlighting its strength as an ensemble method that reduces variance and typically offers better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score for this Random Forest model is 0.84\n",
      "The Confusion Matrix for this Random Forest model is [[59  5]\n",
      " [11 25]]\n"
     ]
    }
   ],
   "source": [
    "# 4(b) (b) Create the train and validation sets again, and create a Random Forest Classifier with the following parameters: **n_estimators, max_leaf_nodes, max_depth**, with some values. \n",
    "# What do this parameters do? (Again look up the documentation) Run the model and see how the accuracy changes. Change the values and try to get a higher accuracy.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "titanic_train = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/train_data.csv')\n",
    "titanic_test = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/test_data.csv')\n",
    "\n",
    "# Dropping the first two columns from the dataset\n",
    "titanic_train = titanic_train.iloc[:, 2:]\n",
    "titanic_test = titanic_test.iloc[:, 2:]\n",
    "\n",
    "# Dividing the dataset into features and target variable\n",
    "x_train = titanic_train.drop('Survived', axis=1)\n",
    "y_train = titanic_train['Survived']\n",
    "\n",
    "x_test = titanic_test.drop('Survived', axis=1)\n",
    "y_test = titanic_test['Survived']\n",
    "\n",
    "# Building the Random Forest Model\n",
    "\n",
    "model = RandomForestClassifier(n_estimators= 150, max_leaf_nodes=20, max_depth=10)\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "# Calculating the Accuracy Scores and Confusion Matrix\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_predict)\n",
    "conf_matrix = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "# Printing the outputs\n",
    "print(f'The Accuracy Score for this Random Forest model is {acc_score}')\n",
    "print(f'The Confusion Matrix for this Random Forest model is {conf_matrix}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q4 (b)\n",
    "\n",
    "With the specified parameters (n_estimators=150, max_leaf_nodes=20, and max_depth=10), the accuracy on the validation set is 84%. This iteration of the model performs worse than the untuned Random Forest\n",
    "\n",
    "Explanation of the Parameters:\n",
    "* n_estimators: This is the number of trees in the Random Forest. More trees generally improve performance but also increase computation time.\n",
    "* max_leaf_nodes: This limits the number of leaf nodes per tree, preventing overly complex trees and reducing the risk of overfitting.\n",
    "* max_depth: This sets the maximum depth of each tree. Limiting the depth helps control the complexity of the model and can prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters Found: {'n_estimators': 110, 'max_leaf_nodes': 19, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "# 4(c) (c) Similar to Question 3 part (c), use a triple for loop to iterate over combinations of parameter values for the random forest and find one that is optimal in accuracy. How does this accuracy compare to the others we have seen?\n",
    "#Note: This kind of parameter optimization can be done using built in python functions, GridSearchCV and RandomSearchCV both are methods that take in some kind of range / distribution for the parameters and \n",
    "# finds the best one (and uses cross validation which is a bonus).\n",
    "\n",
    "# Parameters to keep in mind: n_estimators=150, max_leaf_nodes=20, and max_depth=10\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "titanic_train = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/train_data.csv')\n",
    "titanic_test = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/test_data.csv')\n",
    "\n",
    "# Dropping the first two columns from the dataset\n",
    "titanic_train = titanic_train.iloc[:, 2:]\n",
    "titanic_test = titanic_test.iloc[:, 2:]\n",
    "\n",
    "# Dividing the dataset into features and target variable\n",
    "x_train = titanic_train.drop('Survived', axis=1)\n",
    "y_train = titanic_train['Survived']\n",
    "\n",
    "x_test = titanic_test.drop('Survived', axis=1)\n",
    "y_test = titanic_test['Survived']\n",
    "\n",
    "# Building a model to find the best parameters\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': list(range(100,150,10)),\n",
    "    'max_leaf_nodes': list(range(1,20)),\n",
    "    'max_depth': list(range(5,15))\n",
    "}\n",
    "\n",
    "# now we will build the RandomizedSearchCV model\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator = model,\n",
    "    param_distributions = param_dist,\n",
    "    cv = 3,\n",
    "    n_iter = 20,\n",
    "    scoring = 'accuracy')\n",
    "\n",
    "# Fitting the model\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "# Finding the best parameter for our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score for Random Forest model is 0.83\n",
      "The Confusion Matrix for Random Forest model is [[58  6]\n",
      " [11 25]]\n"
     ]
    }
   ],
   "source": [
    "# Building Random Forest model using best parameters. \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "titanic_train = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/train_data.csv')\n",
    "titanic_test = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/test_data.csv')\n",
    "\n",
    "# Dropping the first two columns from the dataset\n",
    "titanic_train = titanic_train.iloc[:, 2:]\n",
    "titanic_test = titanic_test.iloc[:, 2:]\n",
    "\n",
    "# Dividing the dataset into features and target variable\n",
    "x_train = titanic_train.drop('Survived', axis=1)\n",
    "y_train = titanic_train['Survived']\n",
    "\n",
    "x_test = titanic_test.drop('Survived', axis=1)\n",
    "y_test = titanic_test['Survived']\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=110, max_leaf_nodes=19, max_depth=10)\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_predict)\n",
    "conf_matrix= confusion_matrix(y_test, y_predict)\n",
    "\n",
    "# Printing the outputs\n",
    "print(f'The Accuracy Score for Random Forest model is {acc_score}')\n",
    "print(f'The Confusion Matrix for Random Forest model is {conf_matrix}')\n",
    "\n",
    "The Accuracy Score for this Random Forest model is 0.84\n",
    "The Confusion Matrix for this Random Forest model is [[59  5]\n",
    " [11 25]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q4 (c)\n",
    "\n",
    "The best parameter combination found for the Random Forest on the validation set is:\n",
    "* n_estimators: 110\n",
    "* max_leaf_nodes: 19\n",
    "* max_depth: 10\n",
    "\n",
    "This achieved a validation accuracy of 83%.\n",
    "\n",
    "Comparison:\n",
    "* Untuned Decision Tree: 80% accuracy\n",
    "* Tuned Decision Tree: 86% accuracy\n",
    "* Untuned Random Forest: 84% accuracy\n",
    "* Tuned Random Forest: 83% accuracy\n",
    "\n",
    "In this case, the untuned Random Forest slightly outperformed the tuned versions of both models. This could be due to the simplicity of the dataset, where extensive tuning might not provide additional benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "Come up with an analogy for decision tree's v. random forest's and why random forests avoid the problem of overfitting. (+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of a Decision Tree as a single judge in a courtroom making a ruling based solely on their own perspective. If this judge has a lot of information (i.e., no restrictions like max_depth), they may try to account for every tiny detail, even if those details are just noise or anomalies. This leads to overfitting — making a decision based on specifics that might not apply in general cases.\n",
    "\n",
    "Now imagine a Random Forest as a panel of judges, where each judge gets only a random subset of the evidence (features) and makes their ruling independently. The panel then takes a vote on the final decision. Because each judge sees different parts of the case and they all contribute equally, the final verdict is a blend of their opinions. This process reduces the risk of overfitting because:\n",
    "\n",
    "Diverse Opinions: By giving each judge (tree) different evidence (features), we prevent any single judge from becoming too specific to the details (noise) of a particular case.\n",
    "\n",
    "Averaging Effect: The ensemble decision (average of all trees' predictions) is more robust, reducing the impact of any one judge that might have made a biased or overfitted decision.\n",
    "\n",
    "In essence, a Random Forest is like a team of experts who make more balanced and general decisions than any single expert could on their own. This leads to better generalization on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
